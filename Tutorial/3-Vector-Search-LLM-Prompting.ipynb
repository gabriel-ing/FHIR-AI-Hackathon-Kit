{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1863a11f-dc83-4101-b137-661e87919f08",
   "metadata": {},
   "source": [
    "# 3 - Vector Search and LLM prompting\n",
    "\n",
    "This notebook shows how we can query our vector database using plain-text queries.\n",
    "\n",
    "**Before starting, I want to emphasize that the patient data we are using is all synthetically generated. The main FHIR resources were created by [Synthea](https://synthetichealth.github.io/synthea/), while the clinical notes being used in this example are created by Copilot. \n",
    "\n",
    "As in previous steps, we will first create our iris cursor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13e26fe7-a2f4-4d95-bf86-6be2cfcb4032",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993821d-c6bb-4815-9692-47fabd987de7",
   "metadata": {},
   "source": [
    "The database search is performed using functions built into IRIS-SQL. The query, which is given in plain text, is then encoded into a vector using the same sentence transformer model as used for the information in the database. Here we are going to simply query the database about headaches:\n",
    "\n",
    "Importing the model will take a short time, but encoding the query into embeddings should be quick. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10e59405-4e1a-43e8-a11e-14c67773787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2') \n",
    "query = \"Has the patient reported any chest or respiratory complaints?\"\n",
    "query_vector = model.encode(query, normalize_embeddings=True, show_progress_bar = False).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220badd0-19b8-4d1c-99b8-032396bedc3f",
   "metadata": {},
   "source": [
    "There are two ways to measure the similarity between vectors, we can look at the vector cosine or the dot-product. The cosine is the angle between the two vectors, while the dot product is the distance between them. They have slight differences that I won't go into much detail here, but can be explored if you want to optimise your search. In reality, for small searches, the results will be similar whichever method you use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5939c6-eb17-4c74-bee1-290c29b7f666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------Note Start---------------------------------\n",
      "Date: 2022-06-16\n",
      "Provider: Dr. Fritz267 Lebsack687\n",
      "Location: PCP114062\n",
      "Reason for Visit: Routine general examination\n",
      "Clinical Summary:\n",
      "Alisia attended her scheduled wellness check. She reported feeling generally well, with no acute complaints. Her medical history includes resolved streptococcal pharyngitis and viral sinusitis. She continues to manage her weight and blood pressure effectively.\n",
      "Observations:\n",
      "\n",
      "Height: 172.7 cm\n",
      "Weight: 81.2 kg\n",
      "BMI: 27.23 kg/mÂ²\n",
      "Pain Score: 2/10 (mild discomfort reported in lower back)\n",
      "\n",
      "Assessment:\n",
      "\n",
      "No new diagnoses\n",
      "Stable vitals and weight\n",
      "Mild musculoskeletal discomfort, likely posture-related\n",
      "\n",
      "Plan:\n",
      "\n",
      "Continue current lifestyle and dietary habits\n",
      "Encourage regular stretching and core strengthening exercises\n",
      "Schedule next routine exam in 12 months unless symptoms arise\n",
      "\n",
      "Billing:\n",
      "\n",
      "General examination\n",
      "Submitted to Medicare\n",
      "Total: $129.16\n",
      "--------------------------------------Note End---------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------Note Start---------------------------------\n",
      "Date: 2021-06-20\n",
      "Provider: Dr. Yael855 Hartmann983\n",
      "Location: MILFORD REGIONAL MEDICAL CENTER\n",
      "Reason for Visit: Nasal congestion and facial pressure\n",
      "Clinical Summary:\n",
      "Alisia reported persistent nasal congestion, facial pressure, and postnasal drip lasting over a week. She denied fever, purulent discharge, or severe headache. Physical exam showed mild sinus tenderness and clear nasal secretions. No signs of bacterial infection were present.\n",
      "Diagnosis:\n",
      "\n",
      "Viral sinusitis (active)\n",
      "\n",
      "Treatment Plan:\n",
      "\n",
      "Supportive care: saline nasal irrigation, steam inhalation, and OTC decongestants\n",
      "Avoided antibiotics due to viral etiology\n",
      "Advised to monitor for worsening symptoms or persistence beyond 10 days\n",
      "Provided education on differentiating viral vs. bacterial sinusitis\n",
      "\n",
      "Billing:\n",
      "\n",
      "Encounter for symptom and viral sinusitis\n",
      "Submitted to Medicare\n",
      "Total: $129.16\n",
      "--------------------------------------Note End---------------------------------\n",
      "\n",
      "\n",
      "--------------------------------------Note Start---------------------------------\n",
      "Date: 2021-06-20\n",
      "Provider: Dr. Yael855 Hartmann983\n",
      "Location: MILFORD REGIONAL MEDICAL CENTER\n",
      "Reason for Visit: Nasal congestion and facial pressure\n",
      "Encounter Summary:\n",
      "Alisia returned with complaints of persistent nasal congestion, facial pressure, and postnasal drip lasting over a week. No fever or purulent discharge was noted. Examination showed mild sinus tenderness and clear nasal secretions.\n",
      "Diagnosis:\n",
      "\n",
      "Primary: Viral sinusitis\n",
      "Clinical Status: Active\n",
      "Verification: Based on clinical presentation and absence of bacterial indicators\n",
      "\n",
      "Plan:\n",
      "\n",
      "Supportive care: saline nasal irrigation, steam inhalation, and OTC decongestants\n",
      "Avoid antibiotics unless symptoms worsen or persist beyond 10 days\n",
      "Educated on signs of bacterial superinfection\n",
      "\n",
      "Billing:\n",
      "\n",
      "Encounter for symptom and viral sinusitis\n",
      "Submitted to Medicare\n",
      "Total: $129.16\n",
      "--------------------------------------Note End---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "table_name = \"VectorSearch.DocRefVectors\"\n",
    "search_sql = f\"\"\"\n",
    "        SELECT TOP 3 ClinicalNotes \n",
    "        FROM {table_name}\n",
    "        WHERE PatientID = ?\n",
    "        ORDER BY VECTOR_COSINE(NotesVector, TO_VECTOR(?,double)) DESC\n",
    "    \"\"\"\n",
    "\n",
    "cursor.execute(search_sql, [3, str(query_vector)])\n",
    "results = cursor.fetchall()\n",
    "\n",
    "for result in results:\n",
    "    print(\"\\n--------------------------------------Note Start---------------------------------\")\n",
    "    print(result[0])\n",
    "    print(\"--------------------------------------Note End---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62bdc1c-0268-4526-ac1f-e8cadff5fadf",
   "metadata": {},
   "source": [
    "This SQL query selects the top  result from a vector search of our table. The results are ordered by the vector cosine of the NotesVector column of the database and the vector of our query - we have to order in descending order to get the best match first. \n",
    "\n",
    "Note, we have an additional filter here to only include the results for a particular patient ID which will be provided upon execution. We could provide any number of additional filters, for example each clinical note starts with a date. In the set-up section we could extract this date and save it as a separate column within our data table. Then when querying the database we could add a query to only include a particular date range. \n",
    "\n",
    "Below I have put this into an (almost) standalone function. This function includes the global variables model and table_name defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccb9e00-6dca-4f39-b5b2-737e7dc9007b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_search(user_prompt,patient):\n",
    "    search_vector =  model.encode(user_prompt, normalize_embeddings=True,show_progress_bar=False ).tolist() \n",
    "    \n",
    "    search_sql = f\"\"\"\n",
    "        SELECT TOP 3 ClinicalNotes \n",
    "        FROM {table_name}\n",
    "        WHERE PatientID = ?\n",
    "        ORDER BY VECTOR_COSINE(NotesVector, TO_VECTOR(?,double)) DESC\n",
    "    \"\"\"\n",
    "    cursor.execute(search_sql,[patient, str(search_vector)])\n",
    "    \n",
    "    results = cursor.fetchall()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c31f49dd-7338-48a5-9039-f6927375ebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('Date: 2022-06-16\\nProvider: Dr. Fritz267 Lebsack687\\nLocation: PCP114062\\nReason for Visit: Routine general examination\\nClinical Summary:\\nAlisia attended her scheduled wellness check. She reported feeling generally well, with no acute complaints. Her medical history includes resolved streptococcal pharyngitis and viral sinusitis. She continues to manage her weight and blood pressure effectively.\\nObservations:\\n\\nHeight: 172.7 cm\\nWeight: 81.2 kg\\nBMI: 27.23 kg/mÂ²\\nPain Score: 2/10 (mild discomfort reported in lower back)\\n\\nAssessment:\\n\\nNo new diagnoses\\nStable vitals and weight\\nMild musculoskeletal discomfort, likely posture-related\\n\\nPlan:\\n\\nContinue current lifestyle and dietary habits\\nEncourage regular stretching and core strengthening exercises\\nSchedule next routine exam in 12 months unless symptoms arise\\n\\nBilling:\\n\\nGeneral examination\\nSubmitted to Medicare\\nTotal: $129.16',), ('Date: 2021-06-20\\nProvider: Dr. Yael855 Hartmann983\\nLocation: MILFORD REGIONAL MEDICAL CENTER\\nReason for Visit: Nasal congestion and facial pressure\\nClinical Summary:\\nAlisia reported persistent nasal congestion, facial pressure, and postnasal drip lasting over a week. She denied fever, purulent discharge, or severe headache. Physical exam showed mild sinus tenderness and clear nasal secretions. No signs of bacterial infection were present.\\nDiagnosis:\\n\\nViral sinusitis (active)\\n\\nTreatment Plan:\\n\\nSupportive care: saline nasal irrigation, steam inhalation, and OTC decongestants\\nAvoided antibiotics due to viral etiology\\nAdvised to monitor for worsening symptoms or persistence beyond 10 days\\nProvided education on differentiating viral vs. bacterial sinusitis\\n\\nBilling:\\n\\nEncounter for symptom and viral sinusitis\\nSubmitted to Medicare\\nTotal: $129.16',), ('Date: 2021-06-20\\nProvider: Dr. Yael855 Hartmann983\\nLocation: MILFORD REGIONAL MEDICAL CENTER\\nReason for Visit: Nasal congestion and facial pressure\\nEncounter Summary:\\nAlisia returned with complaints of persistent nasal congestion, facial pressure, and postnasal drip lasting over a week. No fever or purulent discharge was noted. Examination showed mild sinus tenderness and clear nasal secretions.\\nDiagnosis:\\n\\nPrimary: Viral sinusitis\\nClinical Status: Active\\nVerification: Based on clinical presentation and absence of bacterial indicators\\n\\nPlan:\\n\\nSupportive care: saline nasal irrigation, steam inhalation, and OTC decongestants\\nAvoid antibiotics unless symptoms worsen or persist beyond 10 days\\nEducated on signs of bacterial superinfection\\n\\nBilling:\\n\\nEncounter for symptom and viral sinusitis\\nSubmitted to Medicare\\nTotal: $129.16',))\n"
     ]
    }
   ],
   "source": [
    "results = vector_search(query, 3)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d0da42-0b1a-45bb-bbfd-52ce3602f8c5",
   "metadata": {},
   "source": [
    "### Prompting a Local LLM \n",
    "\n",
    "There are a few options when it comes to prompting an LLM. Most LLMs available are accessible through APIs, for example you can use GPT-5 through [OpenAI's API](https://openai.com/api/). For sensitive data, like Patient FHIR data, there might be data protection rules or even laws which would prohibit sending sensitive data to an external API, so this might not be possible. In these cases it is possible to download a local version of models. Local versions can be downloaded with Ollama, or very simply with hugging faces. \n",
    "\n",
    "#### Using hugging faces transformer\n",
    "\n",
    "This is the easiest way to get started with a local LLM. Transformers can be installed using pip and then simply used as shown below.\n",
    "\n",
    "I'm using a very small model to make it lighter-weight for use on my laptop, but don't expect very good results using this method unless you are willing to download larger models. For better results using local models, feel free to skip to the Ollama section below, which requires slightly more set-up but is giving much better results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deaa305c-24b8-49b6-bfa3-87d641299af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56f46ce0-4c92-4ed8-8232-5531d09072f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "# Create our chatbot using the flan-t5-base model\n",
    "generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\", max_length=512, min_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60bd9062-5a40-471d-b7a5-1d9821a961ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (634 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector search complete\n",
      "The patient reported persistent nasal congestion, facial pressure, and postnasal drip lasting over a week. She denied fever, purulent discharge, or severe headache. Physical exam showed mild sinus tenderness and clear nasal secretions. No signs of bacterial infection were present. Examination showed mild sinus tenderness and clear nasal secretions. No signs of bacterial superinfection were present.nDiagnosis:nnViral sinusitis (active)nTreatment Plan:nnSupportive care: saline nasal irrigation, steam inhalation, and OTC decongestantsnAvoid antibiotics due to viral etiologynAdvised to monitor for worsening symptoms or persistence beyond 10 daysnProvided education on differentiating viral vs. bacterial sinusitisnnAdvised to monitor for worsening symptoms or persistence beyond 10 daysnEducated on signs of bacterial superinfectionnnBilling:nnEncounter for symptom and viral sinusitisnSubmitted to MedicarenTotal: $129.16',)\n"
     ]
    }
   ],
   "source": [
    "## Perform Vector Search again\n",
    "query = \"Has the patient reported having bad headaches?\"\n",
    "results = vector_search(query, 3)\n",
    "print(\"Vector search complete\") \n",
    "\n",
    "prompt = (\n",
    "    f\"\"\"SYSTEM: You are a helpful and knowledgeable assistant designed to help a doctor interpret a patient's medical history \n",
    "    using retrieved information from a database. Please provide a detailed and medically relevant explanation and include relevant\n",
    "    dates in your response and ensure your response is coherent \\n\\n\\n\"\"\"\n",
    "    f\"CONTEXT:\\n{results[:-1]}\\n\\n\"\n",
    "    f\"USER QUESTION:\\n{query}\\n\\n\"\n",
    "    f\"ASSISTANT ANSWER:\"\n",
    ")\n",
    "response = generator(prompt)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9968a-24ac-47a3-a654-477a3bb5839f",
   "metadata": {},
   "source": [
    "So the vector search gave an answer which was relevant to the prompt, but ignored some parts of it (notably, I added in the system prompt that it should include dates in its response). This is a limitation of using a small model. \n",
    "\n",
    "### Prompting a local model with Ollama\n",
    "\n",
    "Here we are going to use [Ollama](https://ollama.com/) and in particular the [gemma3:1b](https://ollama.com/library/gemma3) model. To start using Ollama: \n",
    "\n",
    "1. To start using Ollama, download Ollama from the [Ollama Website](https://ollama.com/). \n",
    "2. After downloading Ollama, download the required model \n",
    "    - we are going to use `gemma3:1b`, one of the best small models. If you have very limited computing power or harddrive capacity, feel free to change model to an even smaller one like `gemma3:270m`, which is <300MB in size. For more notes\n",
    "    - To download, select it from the Ollama GUI or use the terminal command `ollama pull gemma3:1b`.\n",
    "3. Download the companion python library: `pip install ollama`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59627af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca85072-a026-4621-8b3b-12e20f929e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================Response Start===============================\n",
      "\n",
      "Okay, let's analyze the provided medical records to address your question.\n",
      "\n",
      "Based on the information available, the patient, Alisia, *does not* report having any history of headaches.\n",
      "\n",
      "Hereâ€™s a breakdown of the relevant information:\n",
      "\n",
      "*   **2021-06-20 (Viral Sinusitis - Active):** The record mentions postnasal drip and clear nasal secretions, which is consistent with a viral infection. Thereâ€™s no mention of headaches.\n",
      "*   **2021-06-20 (Encounter for Symptom and Viral Sinusitis):** The record emphasizes supportive care measures (saline irrigation, steam inhalation, OTC decongestants) â€“ these are treatments for symptoms, not headaches.\n",
      "*   **2022-06-16 (Routine General Examination):** The record focuses on general well-being, weight, and musculoskeletal discomfort. It does not include any information about headaches.\n",
      "\n",
      "**Conclusion:**  The provided records do not indicate that Alisia has reported experiencing headaches.\n",
      "\n",
      "**Disclaimer:** *As an AI assistant, I cannot provide medical diagnoses. This analysis is based solely on the information given and does not constitute a medical evaluation.*\n",
      "\n",
      "Would you like me to elaborate on any specific aspect of this information or provide additional context?\n",
      "\n",
      "==================================Response End===============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "\n",
    "response: ChatResponse = chat(model='gemma3:1b', messages=[{'role': 'system',\n",
    "            'content': (\n",
    "                \"You are a helpful and knowledgeable assistant designed to help a doctor interpret a patient's medical history using retrieved information from a database.\"\n",
    "                \"Please provide a detailed and medically relevant explanation, include relevant dates, and ensure your response is coherent.\"\n",
    "            )},\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': f\"CONTEXT:\\n{results}\\n\\nUSER QUESTION:\\n{query}\"\n",
    "        }])\n",
    "print(\"\\n==================================Response Start===============================\\n\")\n",
    "print(response['message']['content'])\n",
    "print(\"\\n==================================Response End===============================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dd6f8d-9d50-4a83-abb0-c3b2a52d1a87",
   "metadata": {},
   "source": [
    "Thats already a better response than the basic hugging faces one. This could still get much better, as `gemma3:1b` is a very small model (<1Gb in size, 1 billion parameters). This is one of the smallest usable models available. Larger models will generally give better results, whilst also allowing longer prompts with more search results. However, large models and long prompts also comes with the trade-off of requiring increased processing power.\n",
    "\n",
    "Theres  several versions of `gemma3` with increasing numbers of parameters that could give improved results. You can see a list of all available models on the [ollama website](https://ollama.com/library). A shortened list of very small models is availble below: \n",
    "\n",
    "|Model Name|# Parameters (Billions)|Size(GB)|\n",
    "|-|-|-|\n",
    "|gemma3:1b|1|0.8|\n",
    "|gemma3:4b|4|3.3|\n",
    "|gemma3:12b|12|1|\n",
    "|qwen3:0.6b|0.6|0.5|\n",
    "|qwen3:4b|4|2.5|\n",
    "|mistral|7|4.1|\n",
    "|phi4-mini:3.8b|3.8|2.5|\n",
    "\n",
    "For any of these models, you can download them from the Ollama GUI or from the command line with `ollama pull <model-name>`. \n",
    "Different models are good at different things, for example some are specifically good at writing code, others have multi-model capability and can 'look at' images, or 'hear' audio. One thing to consider is that we need a model with a large context window, as this is the amount of information that can be included in the prompt. Sending the data retrieved by the vector search means that our prompts will end up being large in size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb5cd8c-5cbd-4d5d-abda-74337455e4c9",
   "metadata": {},
   "source": [
    "### Adding Memory\n",
    "\n",
    "When we chat with a chatbot, it's ideal for the model to remember the conversation that has come before. Memory can be implemented by passing the previous messages back to the chatbot with our next queries. Instead though, we going to use [LangChain](https://python.langchain.com/docs/introduction/) to automate this. \n",
    "\n",
    "LangChain cane automatically create summaries of the conversations rather than passing the whole message into the chatbot, reducing the length of the prompt to the model. This reduction can speed up the response, and reduce the amount of tokens used (and with API chatbot access, you pay per token). It is common to use a small model as a summariser, with a larger, better model for creating the actual response.\n",
    "\n",
    "The first step is to install langchain and langchain community with pip. Uncomment and run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d4d7f67-42b4-48ae-b94d-c2a06dc83907",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install langchain langchain-ollama "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2496e383-57aa-4ad2-a301-f6904c9ae02b",
   "metadata": {},
   "source": [
    "Here we do the following: \n",
    "    - Import dependencies\n",
    "    - Initialise our model with Ollama\n",
    "    - Create an agent, this agent does not have any \"tools\". Discussing tools and agentic AI is beyond the scope of this tutorial, so for now you can ignore this. \n",
    "    - Initialise agent summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0c84049-0064-4ba5-8c72-af097c5c5607",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, my name is Gabriel\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello Gabriel! It's nice to meet you. How are you today? ðŸ˜Š Is there anything you'd like to chat about?\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "what did I just tell you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "You just told me your name is Gabriel! ðŸ˜„ \n",
      "\n",
      "Thatâ€™s a pretty cool name â€“ itâ€™s a very common name. ðŸ˜Š \n",
      "\n",
      "Is there anything you'd like to tell me about yourself, or would you like to talk about something?\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# Initialize model\n",
    "llm = ChatOllama(model=\"gemma3:1b\") \n",
    "\n",
    "# Initialise short-term memory\n",
    "checkpointer = InMemorySaver()\n",
    "\n",
    "# Create model\n",
    "agent = create_agent(\n",
    "    model=llm, # Set model as our LLM \n",
    "    middleware=[\n",
    "        # create summarization proceedure - this creates summaries of our conversation to keep memory brief.\n",
    "        SummarizationMiddleware(\n",
    "            model=llm,\n",
    "            max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "            messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "        )\n",
    "    ],\n",
    "    # Creates the agent's memory with pre-initialized model\n",
    "    checkpointer=checkpointer,\n",
    ")\n",
    "\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "agent.invoke({\"messages\": [{\"role\":\"user\", \"content\":\"Hello, my name is Gabriel\"}]}, config)\n",
    "final_response = agent.invoke({\"messages\": [{\"role\":\"user\", \"content\":\"what did I just tell you?\"}]}, config)\n",
    "\n",
    "for message in final_response[\"messages\"]:\n",
    "    message.pretty_print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a7b37c-d596-4d5c-8175-ded890356160",
   "metadata": {},
   "source": [
    "The model has memory!\n",
    "\n",
    "### Putting this together\n",
    "\n",
    "Now lets put the whole chat architecture into a single class to make it easy to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f11056d-e0f4-4bab-8842-e5b863810ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import iris\n",
    "class RAGChatbot:\n",
    "    def __init__(self):\n",
    "        self.message_count = 0\n",
    "        conn = iris.connect(\"localhost\", 32782, \"DEMO\", \"_SYSTEM\", \"ISCDEMO\") # Server, Port , Namespace, Username, Password\n",
    "        self.cursor = conn.cursor()\n",
    "        self.agent = self.create_agent()\n",
    "        self.embedding_model = self.get_embedding_model()\n",
    "        \n",
    "\n",
    "    \n",
    "    def get_embedding_model(self):\n",
    "        return  SentenceTransformer('all-MiniLM-L6-v2') \n",
    "        \n",
    "    def create_agent(self):\n",
    "\n",
    "\n",
    "\n",
    "        # Initialize model\n",
    "        llm = ChatOllama(model=\"gemma3:1b\") \n",
    "        \n",
    "        # Initialise short-term memory\n",
    "        checkpointer = InMemorySaver()\n",
    "        \n",
    "        # Create model\n",
    "        agent = create_agent(\n",
    "            model=llm, # Set model as our LLM \n",
    "            middleware=[\n",
    "                # create summarization proceedure - this creates summaries of our conversation to keep memory brief.\n",
    "                SummarizationMiddleware(\n",
    "                    model=llm,\n",
    "                    max_tokens_before_summary=4000,  # Trigger summarization at 4000 tokens\n",
    "                    messages_to_keep=20,  # Keep last 20 messages after summary\n",
    "                )\n",
    "            ],\n",
    "            # Creates the agent's memory with pre-initialized model\n",
    "            checkpointer=checkpointer,\n",
    "        )\n",
    "        self.config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        return agent\n",
    "        \n",
    "    def vector_search(self, user_prompt,patient):\n",
    "        search_vector =  self.embedding_model.encode(user_prompt, normalize_embeddings=True, show_progress_bar=False).tolist() \n",
    "        \n",
    "        search_sql = f\"\"\"\n",
    "            SELECT TOP 3 ClinicalNotes \n",
    "            FROM VectorSearch.DocRefVectors\n",
    "            WHERE PatientID = ?\n",
    "            ORDER BY VECTOR_COSINE(NotesVector, TO_VECTOR(?,double)) DESC\n",
    "        \"\"\"\n",
    "        self.cursor.execute(search_sql,[patient, str(search_vector)])\n",
    "        \n",
    "        results = self.cursor.fetchall()\n",
    "        return results\n",
    "\n",
    "    def run(self):\n",
    "        if self.message_count==0:\n",
    "\n",
    "            \n",
    "            query = input(\"\\n\\nHi, I'm a chatbot used for searching a patient's medical history. How can I help you today? \\n\\n - User: \")\n",
    "        else:\n",
    "            query = input(\"\\n - User:\")\n",
    "        search = True\n",
    "        if self.message_count != 0:\n",
    "            search_ans = input(\"Search the database? [Y/N - default N]\")\n",
    "            if search_ans.lower() != \"y\":\n",
    "                search = False\n",
    "\n",
    "        if search:\n",
    "            try:\n",
    "                patient_id = int(input(\"What is the patient ID?\"))\n",
    "            except:\n",
    "                print(\"The patient ID should be an integer\")\n",
    "                return\n",
    "\n",
    "            results = self.vector_search(query, patient_id)\n",
    "            if results == []:\n",
    "                print(\"No results found, check patient ID\")\n",
    "                return\n",
    "\n",
    "            prompt = f\"CONTEXT:\\n{results}\\n\\nUSER QUESTION:\\n{query}\"\n",
    "        else:\n",
    "            prompt = f\"USER QUESTION:\\n{query}\"\n",
    "\n",
    "        ##print(prompt)\n",
    "        system_prompt = \"You are a helpful and knowledgeable assistant designed to help a doctor interpret a patient's medical history using retrieved information from a database.\\\n",
    "        Please provide a detailed and medically relevant explanation, \\\n",
    "        include the dates of the information you are given.\"\n",
    "        response = self.agent.invoke({\"messages\" : [(\"system\", system_prompt), (\"user\", query), (\"system\", str(results))]}, self.config)\n",
    "        response[\"messages\"][-1].pretty_print()\n",
    "        self.message_count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4b7d85c5-9fd5-47e9-8990-e66d3d64883e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Hi, I'm a chatbot used for searching a patient's medical history. How can I help you today? \n",
      "\n",
      " - User:   Does the patient have any history of respiratory issues? \n",
      "What is the patient ID? 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, letâ€™s analyze this patientâ€™s medical history and determine if thereâ€™s a history of respiratory issues.\n",
      "\n",
      "**Analysis of the Provided Information:**\n",
      "\n",
      "Based on the retrieved data, Alisiaâ€™s history of respiratory issues appears to be **limited but significant**, primarily focused on viral sinusitis. Here's a breakdown:\n",
      "\n",
      "*   **2021-06-20:**  Her initial diagnosis of viral sinusitis, *active*, indicates a recent respiratory issue.  The \"Postnasal drip\" and facial pressure symptoms are consistent with this diagnosis.\n",
      "*   **2021-06-20:** The treatment plan specifically mentions \"viral sinusitis,\" reinforcing this connection.\n",
      "*   **2022-06-16:** The routine examination doesn't mention any respiratory symptoms or history. \n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "The patient's history *primarily* centers around viral sinusitis.  The key takeaway is that she's been diagnosed with this condition, and the treatment plan reflects a management of that specific illness, not a broader respiratory issue.\n",
      "\n",
      "**Therefore, the most relevant information suggesting a possible respiratory history is the 2021-06-20 diagnosis of viral sinusitis.**\n",
      "\n",
      "**Important Note:**  While the data doesnâ€™t explicitly state a respiratory history, itâ€™s crucial to consider that viral sinusitis can, in rare cases, be associated with underlying conditions like immune-mediated sinusitis that *could* involve inflammation and potential respiratory involvement.  However, the current data doesn't indicate a need for further investigation in that area.\n",
      "\n",
      "**To provide a more detailed and medically relevant explanation, I would need to know:**\n",
      "\n",
      "*   **Any previous diagnoses:** Does she have a history of asthma, allergies, or other respiratory conditions?\n",
      "*   **Current symptoms:** Are there any other symptoms sheâ€™s experiencing that might be related to respiratory issues (e.g., shortness of breath, cough, chest pain)?\n",
      "*   **Medications:** Does she take any medications that could affect her respiratory system?\n",
      "\n",
      "Let me know if you'd like me to elaborate on any specific aspect of this analysis or if you have additional information!\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "bot = RAGChatbot()\n",
    "bot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f413e98-1227-4a22-a319-5b8239ade745",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " - User: How about any reports of fatigue? \n",
      "Search the database? [Y/N - default N] y\n",
      "What is the patient ID? 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Okay, letâ€™s break down Alisiaâ€™s fatigue status based on the provided information.\n",
      "\n",
      "**Analysis of Fatigue:**\n",
      "\n",
      "The patientâ€™s fatigue is a significant point to consider, and itâ€™s a layered assessment requiring more context. Hereâ€™s a detailed breakdown:\n",
      "\n",
      "*   **Reported Fatigue:** Alisia *does* report fatigue, specifically mentioning it in her clinical summary. The severity is described as â€œmild,â€ which is important â€“ itâ€™s not a debilitating level.\n",
      "*   **Timing:** The fatigue is *likely* age-related, a crucial piece of information.  Itâ€™s consistent with typical fatigue experienced as individuals age.\n",
      "*   **Associated Symptoms:** Her history of â€œmild joint stiffness in the morningsâ€ adds a layer of potential concern. While joint stiffness itself isn't directly linked to fatigue, it *could* be a symptom of underlying musculoskeletal issues (like postural strain or inflammation), which *could* contribute to fatigue.\n",
      "*   **Lack of Other Symptoms:** The fact that she doesn't report any acute illness makes it harder to immediately rule out a specific underlying cause of fatigue. This allows for a broader exploration.\n",
      "*   **Vitals & Bloodwork:** Her Hemoglobin A1c (6.35%) suggests a slightly elevated blood sugar, which is relevant given the context of fatigue and potential underlying conditions. It also indicates a need for continued monitoring.\n",
      "\n",
      "\n",
      "**Overall Interpretation & Recommendations**\n",
      "\n",
      "Given the context of her fatigue, here's a suggested approach:\n",
      "\n",
      "1.  **Further Investigation:** It is appropriate to request a broader assessment.  Based on her fatigue, I would consider including a:\n",
      "    *   **Comprehensive Fatigue Questionnaire:** This would probe more deeply into the nature of her fatigue â€“ how it impacts her daily life, activities, sleep, etc.\n",
      "    *   **Physical Exam:** A more thorough musculoskeletal examination could help identify any contributing factors.\n",
      "    *   **Repeat HbA1c:** Further monitoring of her glucose control is necessary, especially considering her history of diabetes.\n",
      "\n",
      "2.  **Rule out underlying causes:** Fatigue is a common symptom of several conditions, including:\n",
      "   *   **Vitamin deficiencies**\n",
      "   *   **Thyroid issues**\n",
      "   *   **Sleep disturbances**\n",
      "\n",
      "\n",
      "**In Conclusion:**  The reported fatigue is a key piece of the puzzle, and a more in-depth evaluation is warranted to determine if it's a purely age-related issue or indicative of a more complex situation.\n"
     ]
    }
   ],
   "source": [
    "bot.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "849f7d8e-4dbb-4cdb-b88f-b7a9f68ff9da",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "In this example, we are using synthetically generated clinical notes created by by a large language model. We have used a very small dataset, with only three patients with only 10 clinical notes each. However, it does show the potential of the process. You can hopefully see how this could be scaled up into a useful resource. \n",
    "\n",
    "#### Where to continue from here :\n",
    "\n",
    "If this were a real project, rather than a quick demo, a number of improvements could be made:\n",
    "\n",
    "- #### Vector Search\n",
    "    - **Use a Medical Specific Embedding Model.** \n",
    "    - **Create a Score requirement**\n",
    "        - The relevance of a search result to the prompt can be seen by the vector-cosine with a number between 1 and -1. You can use this to create a minimum requirement to classify a result as a hit.\n",
    "    - **Filter by Date**\n",
    "        - We are querying a patients entire medical history, which means we could get results from 50 years ago which obscure results from 6 months ago due to a slightly better semantic match. In reality we would probably be more interested in recent results, even if they weren't quite as good a match as the older ones.\n",
    "        - We could approach this in different ways - we could just add a date restriction (e.g. search only in last 5 years) or we could take the top N results when ordered by score and then order them by date.\n",
    "\n",
    "- #### LLM prompting\n",
    "    - Use a better LLM\n",
    "    - Refine the system prompt\n",
    "\n",
    "- #### General set-up or design:\n",
    "\n",
    "    - **Link source information**\n",
    "        - It is key that the medical practitioner sees the information that the LLM can see, because LLMs do have a habit of making things up... \n",
    "    - **Add a Front-end User Interface**\n",
    "    - **Create more detailed medical history using a range of resource types (e.g. Conditions, Immunizations, Observations, Medications and more)**\n",
    "    - **Improve Patient ID collection** - At the moment the method to give the Patient ID is very clunky and involves knowing the patient by their ID, not by any easily identifiable systems. This could be improved by adding a beginning form or interaction that asks for full name, address or DOB to identify the patient without needing to know the ID>\n",
    "    - **Using an Agentic method to add functionality to the chatbot** - You could look at LangChain or LangGraph documentation to have a better idea of this. \n",
    "\n",
    "\n",
    "On the final point, here I have used DocumentReference resources because this is one of the few plain text resources with clinically relevant notes. Even in this example, I had to generate the clinical notes myself using an LLM. A more complete example of this may involve creating plain-text strings of medical notes using other resources available. For example, you could group FHIR resources by year, take clinically relevant resources [ Condition, Observation, AllergyIntolerance, Procedures, Immunization, CarePlan], create strings representing the clinical information per year and use this for the vector search method we have implemented above. \n",
    "\n",
    "A nice example of a complete project with a similar set-up is available on the [Open Exchange](https://openexchange.intersystems.com/package/FHIR-Data-Explorer-with-Hybrid-Search-and-AI-Summaries-1) by Pietro Di Leo. There are some key differences from this demo, for example the tabular data is created within Python and directly loaded into a SQL database. \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
